# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1aXpc5t6BsTpARwISoHhVw_pLB9bMrDd5
"""

from google.colab import drive
drive.mount('/content/drive')

# Cell [1]: Import necessary libraries and check PyTorch GPU availability

import torch
import torchvision
import os
from tqdm.notebook import tqdm


# Check if GPU is available
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f'Using device: {device}')

# Print GPU details if available
if torch.cuda.is_available():
    print(f'GPU Name: {torch.cuda.get_device_name(0)}')
    print(f'CUDA Version: {torch.version.cuda}')
    print(f'cuDNN Version: {torch.backends.cudnn.version()}')

import os
import pandas as pd
import matplotlib.pyplot as plt

# Define the main dataset path
dataset_path = r'/content/drive/MyDrive/ML/GuavaDiseaseDataset/GuavaDiseaseDataset'

# Define the subdirectories
subdirs = ['train', 'val', 'test']
classes = ['Anthracnose', 'fruit_fly', 'healthy_guava']

# Initialize a dictionary to store image counts
image_counts = {split: {cls: 0 for cls in classes} for split in subdirs}

# Iterate through each split and class to count images
for split in subdirs:
    for cls in classes:
        cls_dir = os.path.join(dataset_path, split, cls)
        if os.path.exists(cls_dir):
            # Count the number of image files (assuming common image extensions)
            image_files = [f for f in os.listdir(cls_dir)
                           if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif'))]
            image_counts[split][cls] = len(image_files)
        else:
            print(f'Warning: Directory {cls_dir} does not exist.')

# Convert the dictionary to a DataFrame
df_counts = pd.DataFrame(image_counts).T  # Transpose for better readability
print("Image Counts in Each Dataset Split:")
print(df_counts)

# Convert the dictionary to a DataFrame for easier plotting
df_counts_long = df_counts.stack().reset_index()
df_counts_long.columns = ['Dataset Split', 'Class', 'Count']

# Create a bar chart for the image counts at 600 DPI
plt.figure(figsize=(12, 6), dpi=600)
for split in subdirs:
    subset = df_counts_long[df_counts_long['Dataset Split'] == split]
    plt.bar(subset['Class'] + f" ({split})", subset['Count'], label=split)

plt.title('Image Counts in Each Dataset Split')
plt.ylabel('Number of Images')
plt.xlabel('Class and Dataset Split')
plt.xticks(rotation=45, ha='right')
plt.legend(title='Dataset Split')
plt.tight_layout()
plt.show()

# Cell [3]: Define data transformations for training, validation, and testing

from torchvision import transforms

# Define image size (you can adjust based on your GPU memory)
IMAGE_SIZE = 224  # Common size for CNNs

# Define transformations for the training set with data augmentation
train_transforms = transforms.Compose([
    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),
    transforms.RandomHorizontalFlip(),
    transforms.RandomRotation(15),
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406],  # Using ImageNet mean and std
                         [0.229, 0.224, 0.225])
])

# Define transformations for the validation and test sets (no augmentation)
val_test_transforms = transforms.Compose([
    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406],
                         [0.229, 0.224, 0.225])
])

print("Data transformations defined successfully.")

# Cell [4]: Create datasets using ImageFolder

from torchvision import datasets

# Define the main dataset path
dataset_path = r'/content/drive/MyDrive/ML/GuavaDiseaseDataset/GuavaDiseaseDataset'

# Create datasets for training, validation, and testing
train_dataset = datasets.ImageFolder(os.path.join(dataset_path, '/content/drive/MyDrive/ML/GuavaDiseaseDataset/GuavaDiseaseDataset/train'), transform=train_transforms)
val_dataset = datasets.ImageFolder(os.path.join(dataset_path, '/content/drive/MyDrive/ML/GuavaDiseaseDataset/GuavaDiseaseDataset/val'), transform=val_test_transforms)
test_dataset = datasets.ImageFolder(os.path.join(dataset_path, '/content/drive/MyDrive/ML/GuavaDiseaseDataset/GuavaDiseaseDataset/test'), transform=val_test_transforms)

# Print dataset sizes
print(f'Training set size: {len(train_dataset)} images')
print(f'Validation set size: {len(val_dataset)} images')
print(f'Test set size: {len(test_dataset)} images')

# Print class names
print(f'Classes: {train_dataset.classes}')

from torchvision import datasets
import matplotlib.pyplot as plt

# Define the main dataset path
dataset_path = r'/content/drive/MyDrive/ML/GuavaDiseaseDataset/GuavaDiseaseDataset'

# Create datasets for training, validation, and testing
train_dataset = datasets.ImageFolder(os.path.join(dataset_path, '/content/drive/MyDrive/ML/GuavaDiseaseDataset/GuavaDiseaseDataset/train'), transform=train_transforms)
val_dataset = datasets.ImageFolder(os.path.join(dataset_path, '/content/drive/MyDrive/ML/GuavaDiseaseDataset/GuavaDiseaseDataset/val'), transform=val_test_transforms)
test_dataset = datasets.ImageFolder(os.path.join(dataset_path, '/content/drive/MyDrive/ML/GuavaDiseaseDataset/GuavaDiseaseDataset/test'), transform=val_test_transforms)

# Print dataset sizes
train_size = len(train_dataset)
val_size = len(val_dataset)
test_size = len(test_dataset)

print(f'Training set size: {train_size} images')
print(f'Validation set size: {val_size} images')
print(f'Test set size: {test_size} images')

# Print class names
print(f'Classes: {train_dataset.classes}')

# Plot a pie chart for dataset distribution
sizes = [train_size, val_size, test_size]
labels = ['Training', 'Validation', 'Test']
colors = ['#ff9999', '#66b3ff', '#99ff99']

plt.figure(figsize=(8, 8), dpi=600)
plt.pie(sizes, labels=labels, autopct='%1f%%', startangle=90, colors=colors)
plt.title('Dataset Distribution')
plt.show()

# Cell [5]: Initialize data loaders

from torch.utils.data import DataLoader

# Define batch size (adjust based on GPU memory)
BATCH_SIZE = 32

# Initialize data loaders
train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)
val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)
test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)

# Print number of batches in each loader
print(f'Number of training batches: {len(train_loader)}')
print(f'Number of validation batches: {len(val_loader)}')
print(f'Number of test batches: {len(test_loader)}')

import matplotlib.pyplot as plt

# Calculate the total number of batches
total_batches = len(train_loader) + len(val_loader) + len(test_loader)

# Calculate the percentage of each loader
train_percentage = (len(train_loader) / total_batches) * 100
val_percentage = (len(val_loader) / total_batches) * 100
test_percentage = (len(test_loader) / total_batches) * 100

# Display percentages
print(f'Training Data: {train_percentage:.2f}%')
print(f'Validation Data: {val_percentage:.2f}%')
print(f'Test Data: {test_percentage:.2f}%')

# Create a pie chart
labels = ['Training', 'Validation', 'Test']
percentages = [train_percentage, val_percentage, test_percentage]

plt.figure(figsize=(8, 8), dpi=600)
plt.pie(percentages, labels=labels, autopct='%1.1f%%', startangle=90, colors=['skyblue', 'lightgreen', 'salmon'])
plt.title('Data Loader Batch Distribution')
plt.show()

# Cell [6]: Define four CNN models from scratch

import torch.nn as nn
import torch.nn.functional as F

# Model 1: Simple CNN
class SimpleCNN(nn.Module):
    def __init__(self, num_classes=3):
        super(SimpleCNN, self).__init__()
        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)  # Input channels: 3 (RGB), Output channels: 16
        self.pool = nn.MaxPool2d(2, 2)  # Max pooling with kernel size 2
        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)
        self.fc1 = nn.Linear(32 * 56 * 56, 128)  # Adjusted based on image size and pooling
        self.fc2 = nn.Linear(128, num_classes)

    def forward(self, x):
        x = self.pool(F.relu(self.conv1(x)))  # [Batch, 16, 112, 112]
        x = self.pool(F.relu(self.conv2(x)))  # [Batch, 32, 56, 56]
        x = x.view(-1, 32 * 56 * 56)          # Flatten
        x = F.relu(self.fc1(x))
        x = self.fc2(x)
        return x

# Model 2: Deep CNN
class DeepCNN(nn.Module):
    def __init__(self, num_classes=3):
        super(DeepCNN, self).__init__()
        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)
        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)
        self.pool = nn.MaxPool2d(2, 2)
        self.fc1 = nn.Linear(128 * 28 * 28, 256)
        self.fc2 = nn.Linear(256, num_classes)

    def forward(self, x):
        x = self.pool(F.relu(self.conv1(x)))  # [Batch, 32, 112, 112]
        x = self.pool(F.relu(self.conv2(x)))  # [Batch, 64, 56, 56]
        x = self.pool(F.relu(self.conv3(x)))  # [Batch, 128, 28, 28]
        x = x.view(-1, 128 * 28 * 28)         # Flatten
        x = F.relu(self.fc1(x))
        x = self.fc2(x)
        return x

# Model 3: CNN with Batch Normalization
class CNNWithBatchNorm(nn.Module):
    def __init__(self, num_classes=3):
        super(CNNWithBatchNorm, self).__init__()
        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)
        self.bn1 = nn.BatchNorm2d(32)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)
        self.bn2 = nn.BatchNorm2d(64)
        self.pool = nn.MaxPool2d(2, 2)
        self.fc1 = nn.Linear(64 * 56 * 56, 128)
        self.fc2 = nn.Linear(128, num_classes)

    def forward(self, x):
        x = self.pool(F.relu(self.bn1(self.conv1(x))))  # [Batch, 32, 112, 112]
        x = self.pool(F.relu(self.bn2(self.conv2(x))))  # [Batch, 64, 56, 56]
        x = x.view(-1, 64 * 56 * 56)                     # Flatten
        x = F.relu(self.fc1(x))
        x = self.fc2(x)
        return x

# Model 4: CNN with Dropout
class CNNWithDropout(nn.Module):
    def __init__(self, num_classes=3):
        super(CNNWithDropout, self).__init__()
        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)
        self.pool = nn.MaxPool2d(2, 2)
        self.dropout = nn.Dropout(0.5)
        self.fc1 = nn.Linear(64 * 56 * 56, 128)
        self.fc2 = nn.Linear(128, num_classes)

    def forward(self, x):
        x = self.pool(F.relu(self.conv1(x)))  # [Batch, 32, 112, 112]
        x = self.pool(F.relu(self.conv2(x)))  # [Batch, 64, 56, 56]
        x = x.view(-1, 64 * 56 * 56)         # Flatten
        x = F.relu(self.fc1(x))
        x = self.dropout(x)
        x = self.fc2(x)
        return x

# Initialize all four models and move them to the GPU
model1 = SimpleCNN().to(device)
model2 = DeepCNN().to(device)
model3 = CNNWithBatchNorm().to(device)
model4 = CNNWithDropout().to(device)

print("All four models have been defined and moved to the GPU successfully.")

# Cell [7]: Define loss function and optimizers for all four models

import torch.optim as optim

# Define the loss function
criterion = nn.CrossEntropyLoss()

# Define learning rate
learning_rate = 0.001

# Define optimizers for each model
optimizer1 = optim.Adam(model1.parameters(), lr=learning_rate)
optimizer2 = optim.Adam(model2.parameters(), lr=learning_rate)
optimizer3 = optim.Adam(model3.parameters(), lr=learning_rate)
optimizer4 = optim.Adam(model4.parameters(), lr=learning_rate)

print("Loss function and optimizers have been defined successfully.")

# Cell [8]: Implement Early Stopping mechanism

import copy
import torch

class EarlyStopping:
    def __init__(self, patience=5, verbose=False, delta=0):
        """
        Args:
            patience (int): How long to wait after last improvement.
            verbose (bool): If True, prints a message for each validation accuracy improvement.
            delta (float): Minimum change to qualify as an improvement.
        """
        self.patience = patience
        self.verbose = verbose
        self.delta = delta
        self.counter = 0
        self.best_acc = None
        self.early_stop = False
        self.best_model_wts = None

    def __call__(self, val_acc, model):
        if self.best_acc is None:
            self.best_acc = val_acc
            self.best_model_wts = copy.deepcopy(model.state_dict())
            if self.verbose:
                print(f'Initial validation accuracy set to {val_acc:.4f}')
        elif val_acc > self.best_acc + self.delta:
            self.best_acc = val_acc
            self.best_model_wts = copy.deepcopy(model.state_dict())
            self.counter = 0
            if self.verbose:
                print(f'Validation accuracy increased to {val_acc:.4f}. Resetting counter.')
        else:
            self.counter += 1
            if self.verbose:
                print(f'No improvement in validation accuracy for {self.counter} consecutive epochs.')
            if self.counter >= self.patience:
                self.early_stop = True
                if self.verbose:
                    print('Early stopping triggered.')

    def save_checkpoint(self, save_path):
        """Saves the best model weights."""
        if self.best_model_wts is not None:
            torch.save(self.best_model_wts, save_path)
            if self.verbose:
                print(f'Best model weights saved to {save_path}')

# Cell [9]: Initialize EarlyStopping for all four models

# Initialize early stopping for each model
early_stopping1 = EarlyStopping(patience=5, verbose=True)
early_stopping2 = EarlyStopping(patience=5, verbose=True)
early_stopping3 = EarlyStopping(patience=5, verbose=True)
early_stopping4 = EarlyStopping(patience=5, verbose=True)

print("Early stopping mechanisms have been initialized for all models.")

# Cell [10]: Define an enhanced training and validation function with timing and early stopping

import torch.nn as nn
import torch.nn.functional as F
from tqdm.auto import tqdm  # Using tqdm.auto for adaptive progress bars
import time
import copy

def train_model(model, criterion, optimizer, train_loader, val_loader, early_stopping,
               num_epochs=25, save_dir=r'/content/drive/MyDrive/ML/GuavaDiseaseDataset', model_name='Model'):
    """
    Trains the model and validates it at each epoch. Implements early stopping and checkpointing based on validation accuracy.
    Additionally, tracks and displays timing information.

    Args:
        model: The neural network model to train.
        criterion: Loss function.
        optimizer: Optimizer for updating model weights.
        train_loader: DataLoader for training data.
        val_loader: DataLoader for validation data.
        early_stopping: EarlyStopping object.
        num_epochs: Maximum number of epochs to train.
        save_dir: Directory to save model checkpoints.
        model_name: Name identifier for the model (for saving purposes).

    Returns:
        model: Trained model with best weights.
        history: Dictionary containing training and validation loss and accuracy.
    """
    history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': []}
    total_start_time = time.time()  # Record the start time of training

    for epoch in range(num_epochs):
        epoch_start_time = time.time()  # Record the start time of the epoch
        print(f'Epoch {epoch+1}/{num_epochs}')
        print('-' * 30)

        # Each epoch has a training and validation phase
        for phase in ['train', 'val']:
            if phase == 'train':
                model.train()  # Set model to training mode
                dataloader = train_loader
                desc = 'Training'
            else:
                model.eval()   # Set model to evaluation mode
                dataloader = val_loader
                desc = 'Validation'

            running_loss = 0.0
            running_corrects = 0

            # Initialize tqdm progress bar with time tracking
            pbar = tqdm(enumerate(dataloader), total=len(dataloader), desc=desc, leave=False)
            for i, (inputs, labels) in pbar:
                inputs = inputs.to(device)
                labels = labels.to(device)

                # Zero the parameter gradients
                optimizer.zero_grad()

                # Forward pass
                # Track gradients only if in training phase
                with torch.set_grad_enabled(phase == 'train'):
                    outputs = model(inputs)
                    loss = criterion(outputs, labels)
                    _, preds = torch.max(outputs, 1)

                    # Backward pass and optimize only if in training phase
                    if phase == 'train':
                        loss.backward()
                        optimizer.step()

                # Statistics
                running_loss += loss.item() * inputs.size(0)
                running_corrects += torch.sum(preds == labels.data)

                # Update progress bar with batch loss
                pbar.set_postfix({'Batch Loss': loss.item()})

            # Calculate epoch loss and accuracy
            epoch_loss = running_loss / len(dataloader.dataset)
            epoch_acc = running_corrects.double() / len(dataloader.dataset)

            history[f'{phase}_loss'].append(epoch_loss)
            history[f'{phase}_acc'].append(epoch_acc.item())

            print(f'{phase.capitalize()} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')

            # Deep copy the model if in validation phase and using early stopping
            if phase == 'val':
                early_stopping(epoch_acc, model)
                # Save checkpoint if validation accuracy improved
                if epoch_acc > early_stopping.best_acc:
                    checkpoint_path = os.path.join(save_dir, f'{model_name}_best.pth')
                    torch.save(model.state_dict(), checkpoint_path)
                    if early_stopping.verbose:
                        print(f'New best validation accuracy: {epoch_acc:.4f}. Model saved to {checkpoint_path}')
                if early_stopping.early_stop:
                    print("Early stopping triggered.")
                    # Load the best model weights
                    model.load_state_dict(early_stopping.best_model_wts)
                    total_training_time = time.time() - total_start_time
                    print(f"Total Training Time: {format_time(total_training_time)}")
                    return model, history

        # Calculate and display epoch duration
        epoch_duration = time.time() - epoch_start_time
        print(f"Epoch Duration: {format_time(epoch_duration)}")
        print()

    # Load best model weights after training
    model.load_state_dict(early_stopping.best_model_wts)
    total_training_time = time.time() - total_start_time
    print(f"Training complete.")
    print(f"Total Training Time: {format_time(total_training_time)}")
    return model, history

def format_time(seconds):
    """
    Formats time in seconds to hh:mm:ss format.

    Args:
        seconds (float): Time in seconds.

    Returns:
        str: Formatted time string.
    """
    m, s = divmod(seconds, 60)
    h, m = divmod(m, 60)
    return f"{int(h)}h:{int(m)}m:{int(s)}s"

# Cell [11]: Train all four models with enhanced training function (including timing and early stopping)

# Define number of epochs
NUM_EPOCHS = 100

# Define directory to save models (ensure it exists)
save_dir = r'/content/drive/MyDrive/ML/GuavaDiseaseDataset'
os.makedirs(save_dir, exist_ok=True)

# Train Model 1: Simple CNN
print("Training Model 1: Simple CNN")
model1, history1 = train_model(
    model=model1,
    criterion=criterion,
    optimizer=optimizer1,
    train_loader=train_loader,
    val_loader=val_loader,
    early_stopping=early_stopping1,
    num_epochs=NUM_EPOCHS,
    save_dir=save_dir,
    model_name='SimpleCNN'
)
print("Model 1 training complete.\n")

# Train Model 2: Deep CNN
print("Training Model 2: Deep CNN")
model2, history2 = train_model(
    model=model2,
    criterion=criterion,
    optimizer=optimizer2,
    train_loader=train_loader,
    val_loader=val_loader,
    early_stopping=early_stopping2,
    num_epochs=NUM_EPOCHS,
    save_dir=save_dir,
    model_name='DeepCNN'
)
print("Model 2 training complete.\n")

# Train Model 3: CNN with Batch Normalization
print("Training Model 3: CNN with Batch Normalization")
model3, history3 = train_model(
    model=model3,
    criterion=criterion,
    optimizer=optimizer3,
    train_loader=train_loader,
    val_loader=val_loader,
    early_stopping=early_stopping3,
    num_epochs=NUM_EPOCHS,
    save_dir=save_dir,
    model_name='CNNWithBatchNorm'
)
print("Model 3 training complete.\n")

# Train Model 4: CNN with Dropout
print("Training Model 4: CNN with Dropout")
model4, history4 = train_model(
    model=model4,
    criterion=criterion,
    optimizer=optimizer4,
    train_loader=train_loader,
    val_loader=val_loader,
    early_stopping=early_stopping4,
    num_epochs=NUM_EPOCHS,
    save_dir=save_dir,
    model_name='CNNWithDropout'
)
print("Model 4 training complete.\n")

# Cell [12]: Evaluate all four models on the test set

from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix
import numpy as np

def evaluate_model(model, dataloader):
    """
    Evaluates the model on the provided dataloader.

    Args:
        model: Trained PyTorch model.
        dataloader: DataLoader for the test dataset.

    Returns:
        preds (np.array): Predicted class labels.
        labels (np.array): True class labels.
    """
    model.eval()  # Set model to evaluation mode
    all_preds = []
    all_labels = []

    with torch.no_grad():
        for inputs, labels in tqdm(dataloader, desc="Evaluating", leave=False):
            inputs = inputs.to(device)
            labels = labels.to(device)

            outputs = model(inputs)
            _, preds = torch.max(outputs, 1)

            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())

    return np.array(all_preds), np.array(all_labels)

# Evaluate Model 1: Simple CNN
print("Evaluating Model 1: Simple CNN")
preds1, labels1 = evaluate_model(model1, test_loader)
acc1 = accuracy_score(labels1, preds1)
precision1, recall1, f1_score1, _ = precision_recall_fscore_support(labels1, preds1, average='weighted')
cm1 = confusion_matrix(labels1, preds1)
print(f"Accuracy: {acc1:.4f}")
print(f"Precision: {precision1:.4f}")
print(f"Recall: {recall1:.4f}")
print(f"F1-Score: {f1_score1:.4f}\n")

# Evaluate Model 2: Deep CNN
print("Evaluating Model 2: Deep CNN")
preds2, labels2 = evaluate_model(model2, test_loader)
acc2 = accuracy_score(labels2, preds2)
precision2, recall2, f1_score2, _ = precision_recall_fscore_support(labels2, preds2, average='weighted')
cm2 = confusion_matrix(labels2, preds2)
print(f"Accuracy: {acc2:.4f}")
print(f"Precision: {precision2:.4f}")
print(f"Recall: {recall2:.4f}")
print(f"F1-Score: {f1_score2:.4f}\n")

# Evaluate Model 3: CNN with Batch Normalization
print("Evaluating Model 3: CNN with Batch Normalization")
preds3, labels3 = evaluate_model(model3, test_loader)
acc3 = accuracy_score(labels3, preds3)
precision3, recall3, f1_score3, _ = precision_recall_fscore_support(labels3, preds3, average='weighted')
cm3 = confusion_matrix(labels3, preds3)
print(f"Accuracy: {acc3:.4f}")
print(f"Precision: {precision3:.4f}")
print(f"Recall: {recall3:.4f}")
print(f"F1-Score: {f1_score3:.4f}\n")

# Evaluate Model 4: CNN with Dropout
print("Evaluating Model 4: CNN with Dropout")
preds4, labels4 = evaluate_model(model4, test_loader)
acc4 = accuracy_score(labels4, preds4)
precision4, recall4, f1_score4, _ = precision_recall_fscore_support(labels4, preds4, average='weighted')
cm4 = confusion_matrix(labels4, preds4)
print(f"Accuracy: {acc4:.4f}")
print(f"Precision: {precision4:.4f}")
print(f"Recall: {recall4:.4f}")
print(f"F1-Score: {f1_score4:.4f}")

# Cell [13]: Plot training and validation loss and accuracy for all models

import matplotlib.pyplot as plt

def plot_history(history, model_name):
    """
    Plots the training and validation loss and accuracy.

    Args:
        history (dict): Dictionary containing loss and accuracy histories.
        model_name (str): Name of the model for titling the plots.
    """
    epochs = range(1, len(history['train_loss']) + 1)

    plt.figure(figsize=(14, 6), dpi=600)

    # Plot Loss
    plt.subplot(1, 2, 1)
    plt.plot(epochs, history['train_loss'], 'bo-', label='Training Loss')
    plt.plot(epochs, history['val_loss'], 'ro-', label='Validation Loss')
    plt.title(f'{model_name} Loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()

    # Plot Accuracy
    plt.subplot(1, 2, 2)
    plt.plot(epochs, history['train_acc'], 'bo-', label='Training Accuracy')
    plt.plot(epochs, history['val_acc'], 'ro-', label='Validation Accuracy')
    plt.title(f'{model_name} Accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.legend()

    plt.tight_layout()
    plt.show()

# Plot for Model 1: Simple CNN
plot_history(history1, 'Model 1: Simple CNN')

# Plot for Model 2: Deep CNN
plot_history(history2, 'Model 2: Deep CNN')

# Plot for Model 3: CNN with Batch Normalization
plot_history(history3, 'Model 3: CNN with Batch Normalization')

# Plot for Model 4: CNN with Dropout
plot_history(history4, 'Model 4: CNN with Dropout')

# Cell [14]: Plot confusion matrices for all models as percentages

import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np

def plot_confusion_matrix(cm, classes, model_name):
    """
    Plots the confusion matrix as percentages using seaborn heatmap.

    Args:
        cm (np.array): Confusion matrix.
        classes (list): List of class names.
        model_name (str): Name of the model for titling the plot.
    """
    # Normalize the confusion matrix to percentages
    cm_percentage = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100

    plt.figure(figsize=(6, 5), dpi=600)
    sns.heatmap(cm_percentage, annot=True, fmt='.2f', cmap='Blues',
                xticklabels=classes, yticklabels=classes)
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.title(f'Confusion Matrix (Percentage): {model_name}')
    plt.show()

# Get class names
class_names = train_dataset.classes

# Plot for Model 1: Simple CNN
plot_confusion_matrix(cm1, class_names, 'Model 1: Simple CNN')

# Plot for Model 2: Deep CNN
plot_confusion_matrix(cm2, class_names, 'Model 2: Deep CNN')

# Plot for Model 3: CNN with Batch Normalization
plot_confusion_matrix(cm3, class_names, 'Model 3: CNN with Batch Normalization')

# Plot for Model 4: CNN with Dropout
plot_confusion_matrix(cm4, class_names, 'Model 4: CNN with Dropout')

# Cell [15]: Summarize and compare model performances

import pandas as pd

# Create a summary dictionary
summary = {
    'Model': ['Simple CNN', 'Deep CNN', 'CNN with BatchNorm', 'CNN with Dropout'],
    'Accuracy': [acc1, acc2, acc3, acc4],
    'Precision': [precision1, precision2, precision3, precision4],
    'Recall': [recall1, recall2, recall3, recall4],
    'F1-Score': [f1_score1, f1_score2, f1_score3, f1_score4]
}

# Convert to DataFrame
df_summary = pd.DataFrame(summary)

# Display the summary
print("Summary of Model Performances:")
print(df_summary)

# Cell [16]: Save evaluation results and confusion matrices

import pickle

# Define the path to save evaluation metrics
evaluation_save_path = os.path.join(save_dir, 'evaluation_results.pkl')

# Create a dictionary to hold all evaluation metrics
evaluation_results = {
    'SimpleCNN': {
        'accuracy': acc1,
        'precision': precision1,
        'recall': recall1,
        'f1_score': f1_score1,
        'confusion_matrix': cm1
    },
    'DeepCNN': {
        'accuracy': acc2,
        'precision': precision2,
        'recall': recall2,
        'f1_score': f1_score2,
        'confusion_matrix': cm2
    },
    'CNNWithBatchNorm': {
        'accuracy': acc3,
        'precision': precision3,
        'recall': recall3,
        'f1_score': f1_score3,
        'confusion_matrix': cm3
    },
    'CNNWithDropout': {
        'accuracy': acc4,
        'precision': precision4,
        'recall': recall4,
        'f1_score': f1_score4,
        'confusion_matrix': cm4
    }
}

# Save the evaluation metrics using pickle
with open(evaluation_save_path, 'wb') as f:
    pickle.dump(evaluation_results, f)

print(f"All evaluation results have been saved to {evaluation_save_path}.")

# Cell [17]: List all models in memory

import sys

# List all objects in the current global namespace
all_objects = globals()

# Filter objects that are instances of nn.Module (i.e., your models)
models_in_memory = {name: obj for name, obj in all_objects.items() if isinstance(obj, torch.nn.Module)}

print("Models currently loaded in memory:")
for name in models_in_memory:
    print(name)

# Cell [18]: Save all four models manually

import torch
import os

# Define the directory to save models
save_dir = r'/content/drive/MyDrive/ML/saved models'
os.makedirs(save_dir, exist_ok=True)  # Ensure the directory exists

# Define a helper function to save a model
def save_model(model, model_name, save_directory):
    """
    Saves the state_dict of a PyTorch model.

    Args:
        model (nn.Module): The trained model to save.
        model_name (str): A string identifier for the model.
        save_directory (str): The directory where the model will be saved.
    """
    save_path = os.path.join(save_directory, f'{model_name}_best.pth')
    try:
        torch.save(model.state_dict(), save_path)
        print(f"Model '{model_name}' saved successfully at {save_path}.")
    except Exception as e:
        print(f"Error saving model '{model_name}': {e}")

# Save each model
save_model(model1, 'SimpleCNN', save_dir)
save_model(model2, 'DeepCNN', save_dir)
save_model(model3, 'CNNWithBatchNorm', save_dir)
save_model(model4, 'CNNWithDropout', save_dir)

# Cell [19]: List all saved model files in the save directory

import os

save_dir = r'/content/drive/MyDrive/ML/saved models'

# List all .pth files in the directory
saved_models = [f for f in os.listdir(save_dir) if f.endswith('.pth')]

print("Saved Model Files:")
for model_file in saved_models:
    print(model_file)

# Cell [20]: Load and display evaluation results

import pickle
import pandas as pd
import os

# Define the path to the saved evaluation results
evaluation_save_path = r'/content/drive/MyDrive/ML/GuavaDiseaseDataset/evaluation_results.pkl'

# Check if the file exists
if not os.path.exists(evaluation_save_path):
    raise FileNotFoundError(f"The file {evaluation_save_path} does not exist. Please save the evaluation results first.")

# Load the evaluation results
with open(evaluation_save_path, 'rb') as f:  # Correct mode 'rb' for reading a binary file
    evaluation_results = pickle.load(f)

# Create a DataFrame for better visualization
summary = {
    'Model': [],
    'Accuracy': [],
    'Precision': [],
    'Recall': [],
    'F1-Score': []
}

for model_name, metrics in evaluation_results.items():
    summary['Model'].append(model_name)
    summary['Accuracy'].append(metrics['accuracy'])
    summary['Precision'].append(metrics['precision'])
    summary['Recall'].append(metrics['recall'])
    summary['F1-Score'].append(metrics['f1_score'])

# Convert the summary dictionary to a DataFrame
df_summary = pd.DataFrame(summary)

# Display the summary table
print("Summary of Model Performances:")
print(df_summary)

# Cell [21]: Create a performance comparison plot

import matplotlib.pyplot as plt
import seaborn as sns
import pickle
import pandas as pd

# Load the evaluation results
with open(evaluation_save_path, 'rb') as f:
    evaluation_results = pickle.load(f)

# Create a DataFrame
summary = {
    'Model': [],
    'Accuracy': [],
    'Precision': [],
    'Recall': [],
    'F1-Score': []
}

for model_name, metrics in evaluation_results.items():
    summary['Model'].append(model_name)
    summary['Accuracy'].append(metrics['accuracy'])
    summary['Precision'].append(metrics['precision'])
    summary['Recall'].append(metrics['recall'])
    summary['F1-Score'].append(metrics['f1_score'])

df_summary = pd.DataFrame(summary)

# Set the plot style
sns.set(style="whitegrid")

# Plot Accuracy, Precision, Recall, and F1-Score for each model
fig, axes = plt.subplots(2, 2, figsize=(15, 10), dpi=600)

metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']
axes = axes.flatten()

for idx, metric in enumerate(metrics):
    sns.barplot(x='Model', y=metric, data=df_summary, ax=axes[idx], palette='viridis')
    axes[idx].set_title(f'{metric} Comparison')
    axes[idx].set_ylim(0, 1)  # Assuming metrics are between 0 and 1
    for p in axes[idx].patches:
        axes[idx].annotate(f"{p.get_height():.2f}", (p.get_x() + p.get_width() / 2., p.get_height()),
                           ha='center', va='bottom', fontsize=11, color='black', xytext=(0, 5),
                           textcoords='offset points')

plt.tight_layout()
plt.show()

import torch
import os

# Define the file path to save the best model
evaluation_save_dir = r'/content/drive/MyDrive/ML/GuavaDiseaseDataset'
os.makedirs(evaluation_save_dir, exist_ok=True)  # Ensure the directory exists

# Define the best model (replace 'DeepCNN' with your chosen model)
best_model = model2  # Example: DeepCNN
best_model_name = 'DeepCNN_best'

# Define the save path for the model
best_model_path = os.path.join(evaluation_save_dir, f'{best_model_name}.pth')

# Save the state_dict
try:
    torch.save(best_model.state_dict(), best_model_path)
    print(f"Best model '{best_model_name}' saved successfully at {best_model_path}.")
except Exception as e:
    print(f"Error saving the best model '{best_model_name}': {e}")

# Ensure all models are in evaluation mode
models = [model1, model2, model3, model4]

# Define a function to evaluate a single model
def evaluate_model(model, test_loader):
    """
    Evaluates the model on the test dataset.
    Args:
        model: Trained PyTorch model.
        test_loader: DataLoader for the test dataset.

    Returns:
        accuracy, precision, recall, f1_score
    """
    model.eval()  # Set model to evaluation mode
    all_preds = []
    all_labels = []

    with torch.no_grad():
        for inputs, labels in test_loader:
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs)
            _, preds = torch.max(outputs, 1)
            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())

    # Calculate evaluation metrics
    accuracy = accuracy_score(all_labels, all_preds)
    precision, recall, f1_score, _ = precision_recall_fscore_support(all_labels, all_preds, average='weighted')
    return accuracy, precision, recall, f1_score

# Evaluate all models and store results
results = []
for idx, model in enumerate(models, start=1):
    print(f"Evaluating Model {idx}...")
    accuracy, precision, recall, f1_score = evaluate_model(model, test_loader)
    results.append({
        'Model': f"Model_{idx}",
        'Accuracy': accuracy,
        'Precision': precision,
        'Recall': recall,
        'F1-Score': f1_score
    })

# Convert results to a DataFrame for better visualization
results_df = pd.DataFrame(results)

# Print and display the performance of all models
print("Model Performance on Test Dataset:")
print(results_df)

# Identify the best model based on accuracy
best_model_idx = results_df['Accuracy'].idxmax()
best_model_name = results_df.loc[best_model_idx, 'Model']
best_model_accuracy = results_df.loc[best_model_idx, 'Accuracy']
print(f"\nThe best model is {best_model_name} with an accuracy of {best_model_accuracy:.4f}.")

# Ensure all models are in evaluation mode
models = [model1, model2, model3, model4]

# Define a function to evaluate a single model
def evaluate_model(model, test_loader):
    """
    Evaluates the model on the test dataset.
    Args:
        model: Trained PyTorch model.
        test_loader: DataLoader for the test dataset.

    Returns:
        accuracy, precision, recall, f1_score
    """
    model.eval()  # Set model to evaluation mode
    all_preds = []
    all_labels = []

    with torch.no_grad():
        for inputs, labels in test_loader:
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs)
            _, preds = torch.max(outputs, 1)
            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())

    # Calculate evaluation metrics
    accuracy = accuracy_score(all_labels, all_preds)
    precision, recall, f1_score, _ = precision_recall_fscore_support(all_labels, all_preds, average='weighted')
    return accuracy, precision, recall, f1_score

# Evaluate all models and store results
results = []
for idx, model in enumerate(models, start=1):
    print(f"Evaluating Model {idx}...")
    accuracy, precision, recall, f1_score = evaluate_model(model, test_loader)
    results.append({
        'Model': f"Model_{idx}",
        'Accuracy': accuracy,
        'Precision': precision,
        'Recall': recall,
        'F1-Score': f1_score
    })

# Convert results to a DataFrame for better visualization
results_df = pd.DataFrame(results)

# Print the performance of all models
print("Model Performance on Test Dataset:")
print(results_df)

# Identify the best model based on accuracy
best_model_idx = results_df['Accuracy'].idxmax()
best_model_name = results_df.loc[best_model_idx, 'Model']
best_model_accuracy = results_df.loc[best_model_idx, 'Accuracy']
print(f"\nThe best model is {best_model_name} with an accuracy of {best_model_accuracy:.4f}.")

# Plot the metrics for all models
import matplotlib.pyplot as plt
import seaborn as sns

# Set the plot style
sns.set(style="whitegrid")

# Metrics to plot
metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']

# Create a figure for the plots at 600 DPI
plt.figure(figsize=(12, 8), dpi=600)

# Plot each metric
for i, metric in enumerate(metrics, 1):
    plt.subplot(2, 2, i)
    sns.barplot(x='Model', y=metric, data=results_df, palette='viridis')
    plt.title(f'{metric} Comparison')
    plt.ylabel(metric)
    plt.ylim(0, 1)  # Metrics range between 0 and 1
    for p in plt.gca().patches:
        plt.gca().annotate(f"{p.get_height():.2f}", (p.get_x() + p.get_width() / 2., p.get_height()),
                           ha='center', va='bottom', fontsize=10, color='black', xytext=(0, 5),
                           textcoords='offset points')

plt.tight_layout()
plt.show()

# Cell [24]: Visualize sample predictions

import torch
from PIL import Image
from torchvision import transforms
import matplotlib.pyplot as plt
import os

# Define image transformations (must match training transformations)
test_transforms = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406],
                         [0.229, 0.224, 0.225])
])

# Function to predict the class of a single image
def predict_image(image_path, model, transform, class_names):
    """
    Predicts the class of an image using the trained model.

    Args:
        image_path (str): Path to the image file.
        model (nn.Module): Trained PyTorch model.
        transform (transforms.Compose): Transformations to apply to the image.
        class_names (list): List of class names.

    Returns:
        str: Predicted class name.
        float: Confidence score of the prediction.
    """
    image = Image.open(image_path).convert('RGB')
    image_tensor = transform(image)
    image_tensor = image_tensor.unsqueeze(0)  # Add batch dimension
    image_tensor = image_tensor.to(device)

    with torch.no_grad():
        outputs = model(image_tensor)
        probabilities = torch.nn.functional.softmax(outputs, dim=1)
        confidence, preds = torch.max(probabilities, 1)

    return class_names[preds.item()], confidence.item()

# Function to display image with prediction
def display_prediction(image_path, model, transform, class_names):
    predicted_class, confidence = predict_image(image_path, model, transform, class_names)
    image = Image.open(image_path).convert('RGB')

    # Render the plot with 600 DPI
    plt.figure(dpi=600)
    plt.imshow(image)
    plt.title(f'Predicted: {predicted_class} ({confidence*100:.2f}%)')
    plt.axis('off')
    plt.show()

# Directory containing test images
test_images_dir = os.path.join(dataset_path, 'test')

# Get a list of test image paths
test_image_files = [os.path.join(test_images_dir, cls, f)
                   for cls in class_names
                   for f in os.listdir(os.path.join(test_images_dir, cls))
                   if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif'))]

# Select a few random images to display predictions
import random

sample_images = random.sample(test_image_files, 30)  # Display 30 sample images

# Choose the best model (e.g., DeepCNN)
best_model = model2  # Replace with your chosen model

for img_path in sample_images:
    display_prediction(img_path, best_model, test_transforms, class_names)

# Cell [25]: Summarize Hyperparameter Settings

import pandas as pd

# Define hyperparameters
hyperparameters = {
    'Hyperparameter': ['Batch Size', 'Learning Rate', 'Number of Epochs', 'Optimizer', 'Early Stopping Patience'],
    'Value': [32, 0.001, 10, 'Adam', 5]
}

# Create a DataFrame
df_hyperparams = pd.DataFrame(hyperparameters)

# Display the hyperparameters
print("Hyperparameter Settings:")
print(df_hyperparams)

# Cell [26]: Compute and Plot ROC Curves for All Models

import torch
from sklearn.metrics import roc_curve, auc
from sklearn.preprocessing import label_binarize
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
from tqdm import tqdm

# Ensure all models are in evaluation mode
models = {
    'SimpleCNN': model1,
    'DeepCNN': model2,
    'CNNWithBatchNorm': model3,
    'CNNWithDropout': model4
}

for model in models.values():
    model.eval()

# Initialize dictionaries to store true labels and predicted probabilities
true_labels = []
pred_probs = {model_name: [] for model_name in models.keys()}

# Iterate over the test loader and collect true labels and predicted probabilities
with torch.no_grad():
    for inputs, labels in tqdm(test_loader, desc="Collecting predictions"):
        inputs = inputs.to(device)
        labels = labels.to(device)
        true_labels.extend(labels.cpu().numpy())

        for model_name, model in models.items():
            outputs = model(inputs)
            probabilities = torch.nn.functional.softmax(outputs, dim=1)
            pred_probs[model_name].extend(probabilities.cpu().numpy())

# Binarize the true labels for multi-class ROC
num_classes = len(classes)
true_binarized = label_binarize(true_labels, classes=range(num_classes))

# Initialize plot with 600 DPI
fig, axes = plt.subplots(1, num_classes, figsize=(5*num_classes, 5), dpi=600)
if num_classes == 1:
    axes = [axes]  # Ensure axes is iterable

# Colors for different models
colors = {
    'SimpleCNN': 'blue',
    'DeepCNN': 'green',
    'CNNWithBatchNorm': 'red',
    'CNNWithDropout': 'purple'
}

for i, class_name in enumerate(classes):
    ax = axes[i]
    for model_name in models.keys():
        # Get the predicted probabilities for the current class
        probs = [prob[i] for prob in pred_probs[model_name]]

        # Compute ROC curve and ROC area for the class
        fpr, tpr, _ = roc_curve(true_binarized[:, i], probs)
        roc_auc = auc(fpr, tpr)

        # Plot ROC curve
        ax.plot(fpr, tpr, color=colors[model_name],
                lw=2, label=f'{model_name} (AUC = {roc_auc:.2f})')

    # Plot the diagonal (random chance)
    ax.plot([0, 1], [0, 1], 'k--', lw=2)
    ax.set_xlim([0.0, 1.0])
    ax.set_ylim([0.0, 1.05])
    ax.set_xlabel('False Positive Rate')
    ax.set_ylabel('True Positive Rate')
    ax.set_title(f'ROC Curve for class: {class_name}')
    ax.legend(loc="lower right")

plt.tight_layout()
plt.show()

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Example metrics for your 4 models
# Replace these with actual values from your models
model_metrics = {
    'Model': ['Model 1', 'Model 2', 'Model 3', 'Model 4'],
    'Accuracy': [0.92, 0.89, 0.87, 0.91],
    'Precision': [0.91, 0.88, 0.86, 0.90],
    'Recall': [0.93, 0.90, 0.88, 0.92],
    'F1-Score': [0.92, 0.89, 0.87, 0.91]
}

# Convert metrics to a pandas DataFrame
df_metrics = pd.DataFrame(model_metrics)

# Transpose the DataFrame for heatmap
df_heatmap = df_metrics.set_index('Model').T

# Plot the heatmap
plt.figure(figsize=(8, 6), dpi=600)
sns.heatmap(df_heatmap, annot=True, cmap='coolwarm', fmt=".2f", linewidths=0.5)
plt.title("Model Performance Metrics Heatmap")
plt.ylabel("Metrics")
plt.xlabel("Models")
plt.show()

import matplotlib.pyplot as plt
import matplotlib.patches as mpatches

# Function to create a stylish and spacious methodology flowchart
def draw_stylish_methodology_flowchart():
    fig, ax = plt.subplots(figsize=(12, 12), dpi=600)  # High resolution with more space

    # Define steps and their positions
    steps = [
        "Dataset Preparation",
        "Preprocessing",
        "Model Architectures",
        "Training Procedure",
        "Evaluation Metrics",
        "Visualization"
    ]
    box_positions = [(0.5, 0.95), (0.5, 0.8), (0.5, 0.65), (0.5, 0.5), (0.5, 0.35), (0.5, 0.2)]

    # Define colors for boxes (gradient-like colors)
    colors = ['#76c7c0', '#88d498', '#f5dd90', '#fcab64', '#ec524b', '#9d5c63']

    # Add boxes for steps with a more stylish look
    for i, (step, pos, color) in enumerate(zip(steps, box_positions, colors)):
        ax.add_patch(mpatches.FancyBboxPatch(
            (pos[0] - 0.3, pos[1] - 0.06), 0.6, 0.12,  # Larger boxes for clarity
            boxstyle="round,pad=0.4,rounding_size=0.2", edgecolor='black', facecolor=color, linewidth=2
        ))
        ax.text(pos[0], pos[1], step, fontsize=14, ha='center', va='center', fontweight='bold', color="white")

    # Add arrows between steps with a smooth curve
    for i in range(len(box_positions) - 1):
        start = box_positions[i]
        end = box_positions[i + 1]
        ax.annotate("", xy=(start[0], start[1] - 0.06), xytext=(end[0], end[1] + 0.06),
                    arrowprops=dict(arrowstyle="-|>", lw=2, color="gray", mutation_scale=15))

    # Add details for each step with stylish formatting
    details = {
        "Dataset Preparation": "Kaggle Guava Dataset\nClasses: Anthracnose, Fruit Fly, Healthy Guava",
        "Preprocessing": "Resize, Normalize, Augment",
        "Model Architectures": "Simple CNN, Deep CNN,\nBatchNorm CNN, Dropout CNN",
        "Training Procedure": "CrossEntropyLoss, Adam Optimizer,\nEarly Stopping",
        "Evaluation Metrics": "Accuracy, Precision, Recall,\nF1-Score, Confusion Matrix, ROC",
        "Visualization": "Loss/Accuracy Curves,\nConfusion Matrices, Performance Heatmap"
    }

    detail_positions = [
        (0.5, 0.9), (0.5, 0.75), (0.5, 0.6),
        (0.5, 0.45), (0.5, 0.3), (0.5, 0.15)
    ]

    for step, pos in zip(details.values(), detail_positions):
        ax.text(pos[0], pos[1], step, fontsize=10, ha='center', va='center', color="darkblue", style='italic')

    # Add title and customize layout
    plt.title("Stylish Methodology Flowchart", fontsize=20, fontweight='bold', color="#37474F", pad=30)
    ax.set_xlim(0, 1)
    ax.set_ylim(0, 1)
    ax.axis('off')  # Hide the axes for a clean look

    # Save as a high-resolution image
    plt.savefig("Stylish_Methodology_Flowchart.png", bbox_inches='tight', dpi=600)
    plt.show()

# Call the function to draw the stylish flowchart
draw_stylish_methodology_flowchart()